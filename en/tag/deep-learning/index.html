<!doctype html><!-- This site was created with Hugo Blox. https://hugoblox.com --><!-- Last Published: March 6, 2025 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Hugo Blox Builder 5.9.7"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.26c458e6907dc03073573976b7f4044e.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.f6689966c0a10712f95f034011917db0.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Jiahao Zhang"><meta name=description content="Academic Page abouth Jiahao Zhang"><link rel=alternate hreflang=zh href=https://academic.davidz.cn/zh/tag/deep-learning/><link rel=alternate hreflang=en-us href=https://academic.davidz.cn/en/tag/deep-learning/><link rel=canonical href=https://academic.davidz.cn/en/tag/deep-learning/><link rel=manifest href=/en/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@academic-davidz"><meta property="twitter:creator" content="@academic-davidz"><meta property="twitter:image" content="https://academic.davidz.cn/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="og:type" content="website"><meta property="og:site_name" content="Academic-DavidZ"><meta property="og:url" content="https://academic.davidz.cn/en/tag/deep-learning/"><meta property="og:title" content="Deep Learning | Academic-DavidZ"><meta property="og:description" content="Academic Page abouth Jiahao Zhang"><meta property="og:image" content="https://academic.davidz.cn/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2025-02-27T00:00:00+00:00"><link rel=alternate href=/en/tag/deep-learning/index.xml type=application/rss+xml title=Academic-DavidZ><title>Deep Learning | Academic-DavidZ</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper><script src=/js/wowchemy-init.min.9e4214442a7711d35691acd58f6f6361.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/en/>Academic-DavidZ</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/en/>Academic-DavidZ</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/en/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/en/#featured><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/en/#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/en/#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=/en/#awards><span>Awards</span></a></li><li class=nav-item><a class=nav-link href=https://cv.davidz.cn/ target=_blank rel=noopener><span>Curriculum Vitae</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li><li class="nav-item dropdown i18n-dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true aria-label=Languages><i class="fas fa-globe mr-1" aria-hidden=true></i><span class="d-none d-lg-inline">English</span></a><div class=dropdown-menu><div class="dropdown-item dropdown-item-active"><span>English</span></div><a class=dropdown-item href=https://academic.davidz.cn/zh/tag/deep-learning/><span>中文 (简体)</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class="universal-wrapper pt-3"><h1>Deep Learning</h1></div><div class=universal-wrapper><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/en/publication/deng-cvpr-2025/>Pos3R: 6D Pose Estimation for Unseen Objects Made Easy</a></div><a href=/en/publication/deng-cvpr-2025/ class=summary-link><div class=article-style>Foundation models have significantly reduced the need for task-specific training, while also enhancing generalizability. However, …</div></a><div class="stream-meta article-metadata"><div><span>Weijian Deng</span>, <span>Dylan Campbell</span>, <span>Chunyi Sun</span>, <span class=author-highlighted>Jiahao Zhang</span>, <span>Shubham Kanitkar</span>, <span>Matthew E. Shaffer</span>, <span>Stephen Gould</span></div></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/en/publication/deng-cvpr-2025/cite.bib>Cite</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=https://github.com/DavidZhang73/Manual-PA target=_blank rel=noopener>Manual-PA</a></div><a href=https://github.com/DavidZhang73/Manual-PA target=_blank rel=noopener class=summary-link><div class=article-style>Official implementation of Manual-PA: Learning 3D Part Assembly from Instruction Diagrams.</div></a><div class="stream-meta article-metadata"></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/DavidZhang73/Manual-PA target=_blank rel=noopener>Code</a></div></div><div class=ml-3><a href=https://github.com/DavidZhang73/Manual-PA target=_blank rel=noopener><img src=/en/project/manual-pa/featured_huaaffbd521d97088514c514884367b2b6_185219_c7a1c8afd83e33e4625bb6b61c99a203.webp height=71 width=150 alt=Manual-PA loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=https://github.com/DavidZhang73/TDGV target=_blank rel=noopener>TDGV</a></div><a href=https://github.com/DavidZhang73/TDGV target=_blank rel=noopener class=summary-link><div class=article-style>Official implementation of WACV 2025 Temporal Instructional Diagram Grounding in Unconstrained Videos.</div></a><div class="stream-meta article-metadata"></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/DavidZhang73/TDGV target=_blank rel=noopener>Code</a></div></div><div class=ml-3><a href=https://github.com/DavidZhang73/TDGV target=_blank rel=noopener><img src=/en/project/tdgv/featured_hu81a49bbe34b252829c0d9263ba11984f_178158_f0d562fff67d51a916fdf792213ae115.webp height=58 width=150 alt=TDGV loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/en/publication/zhang-arxiv-2025/>Manual-PA: Learning 3D Part Assembly from Instruction Diagrams</a></div><a href=/en/publication/zhang-arxiv-2025/ class=summary-link><div class=article-style>This paper presents a transformer-based framework that leverages instructional diagrams to guide 3D part assembly, addressing challenges in sequencing and pose estimation. Using contrastive learning and cross-modal attention, it aligns 2D manual steps with 3D parts, predicts assembly order, and refines poses, achieving state-of-the-art performance on PartNet and IKEA-Manual datasets. The method demonstrates strong generalization to real-world scenarios, significantly improving accuracy and robustness in automated assembly tasks. (Generated by ChatGPT4o).</div></a><div class="stream-meta article-metadata"><div><span class=author-highlighted>Jiahao Zhang</span>, <span>Anoop Cherian</span>, <span>Cristian Rodriguez</span>, <span>Weijian Deng</span>, <span>Stephen Gould</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2411.18011 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/en/publication/zhang-arxiv-2025/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/DavidZhang73/Manual-PA target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2411.18011 target=_blank rel=noopener>ArXiv</a></div></div><div class=ml-3><a href=/en/publication/zhang-arxiv-2025/><img src=/en/publication/zhang-arxiv-2025/featured_huc624b6be12134b14bcc82098be4060bc_208447_94830cd4f9876d7945680a7b54cd70ed.webp height=84 width=150 alt="Manual-PA: Learning 3D Part Assembly from Instruction Diagrams" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/en/publication/zhang-wacv-2025/>Temporally Grounding Instructional Diagrams in Unconstrained Videos</a></div><a href=/en/publication/zhang-wacv-2025/ class=summary-link><div class=article-style>This paper introduces a method for simultaneously localizing multiple instructional diagram queries in videos, addressing the limitations of current approaches that handle queries individually. The proposed method uses composite queries combining visual features and positional embeddings, reducing overlaps and correcting temporal misalignment. Tested on the IAW and YouCook2 datasets, this approach significantly improves grounding accuracy by leveraging self-attention and cross-attention mechanisms, outperforming existing methods while maintaining the temporal structure of instructional steps. (Generated by ChatGPT4o).</div></a><div class="stream-meta article-metadata"><div><span class=author-highlighted>Jiahao Zhang</span>, <span>Frederic Zhang</span>, <span>Cristian Rodriguez</span>, <span>Yizhak Ben-Shabat</span>, <span>Anoop Cherian</span>, <span>Stephen Gould</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content/WACV2025/papers/Zhang_Temporally_Grounding_Instructional_Diagrams_in_Unconstrained_Videos_WACV_2025_paper.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/en/publication/zhang-wacv-2025/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/DavidZhang73/TDGV target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/media/wacv25-126.pdf target=_blank rel=noopener>Poster</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2407.12066 target=_blank rel=noopener>ArXiv</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://huggingface.co/datasets/DavidZhang73/TDGVDatasets target=_blank rel=noopener>Dataset</a></div></div><div class=ml-3><a href=/en/publication/zhang-wacv-2025/><img src=/en/publication/zhang-wacv-2025/featured_huf33ce902d26e13d5970a88b7113bb1eb_1108386_2976f462c2140895a7c228b42ea4ee00.webp height=50 width=150 alt="Temporally Grounding Instructional Diagrams in Unconstrained Videos" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=https://github.com/DavidZhang73/AssemblyVideoManualAlignment target=_blank rel=noopener>Assembly Video Manual Alignment</a></div><a href=https://github.com/DavidZhang73/AssemblyVideoManualAlignment target=_blank rel=noopener class=summary-link><div class=article-style>Official implementation of CVPR 2023 Aligning Step-by-Step Instructional Diagrams to Video Demonstrations.</div></a><div class="stream-meta article-metadata"></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/DavidZhang73/AssemblyVideoManualAlignment target=_blank rel=noopener>Code</a></div></div><div class=ml-3><a href=https://github.com/DavidZhang73/AssemblyVideoManualAlignment target=_blank rel=noopener><img src=/en/project/assembly-video-manual-alignment/featured_hu0841f9d955ced15ce79790f0bb5508fd_132634_dcd1b8e79352e909525e936212f158ef.webp height=60 width=150 alt="Assembly Video Manual Alignment" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/en/publication/zhang-cvpr-2023/>Aligning Step-by-Step Instructional Diagrams to Video Demonstrations</a></div><a href=/en/publication/zhang-cvpr-2023/ class=summary-link><div class=article-style>This paper introduces a supervised contrastive learning approach that learns to align videos with the subtle details of assembly diagrams, guided by a set of novel losses. To study this problem and evaluate the effectiveness of their method, they introduce a new dataset: IAW—for Ikea assembly in the wild—consisting of 183 hours of videos from diverse furniture assembly collections and nearly 8,300 illustrations from their associated instruction manuals and annotated for their ground truth alignments. They define two tasks on this dataset: First, nearest neighbor retrieval between video segments and illustrations, and, second, alignment of instruction steps and the segments for each video. Extensive experiments on IAW demonstrate superior performance of their approach against alternatives. (Generated by New Bing).</div></a><div class="stream-meta article-metadata"><div><span class=author-highlighted>Jiahao Zhang</span>, <span>Anoop Cherian</span>, <span>Yanbin Liu</span>, <span>Yizhak Ben-Shabat</span>, <span>Cristian Rodriguez</span>, <span>Stephen Gould</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Aligning_Step-by-Step_Instructional_Diagrams_to_Video_Demonstrations_CVPR_2023_paper.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/en/publication/zhang-cvpr-2023/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/DavidZhang73/AssemblyVideoManualAlignment target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://iaw.davidz.cn target=_blank rel=noopener>Dataset</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://cvpr2023.thecvf.com/media/PosterPDFs/CVPR%202023/22280.png target=_blank rel=noopener>Poster</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://cvpr2023.thecvf.com/media/cvpr-2023/Slides/22280.pdf target=_blank rel=noopener>Slides</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=8iC5QyP8U6o" target=_blank rel=noopener>Video</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/CVPR52729.2023.00245 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2303.13800 target=_blank rel=noopener>ArXiv</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/en/publication/zhang-cvpr-2023/supplementary.pdf>Supplementary</a></div></div><div class=ml-3><a href=/en/publication/zhang-cvpr-2023/><img src=/en/publication/zhang-cvpr-2023/featured_hu47d4830ae752520931f9bd070df3961b_714524_4d94f7642c4f078616285f6282fdcb97.webp height=66 width=150 alt="Aligning Step-by-Step Instructional Diagrams to Video Demonstrations" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/en/publication/zhuang-iros-2022/>GoferBot: A Visual Guided Human-Robot Collaborative Assembly System</a></div><a href=/en/publication/zhuang-iros-2022/ class=summary-link><div class=article-style>GoferBot is a novel assembly system that seamlessly integrates all sub-modules by utilising implicit semantic information purely from visual perception.</div></a><div class="stream-meta article-metadata"><div><span>Zheyu Zhuang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Yizhak Ben-Shabat</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span class=author-highlighted>Jiahao Zhang</span>, <span>Stephen Gould</span>, <span>Robert Mahony</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://ieeexplore.ieee.org/abstract/document/9981122 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/en/publication/zhuang-iros-2022/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=Fo5XI5OJ4QQ" target=_blank rel=noopener>Video</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/IROS47612.2022.9981122 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2304.08840 target=_blank rel=noopener>ArXiv</a></div></div><div class=ml-3><a href=/en/publication/zhuang-iros-2022/><img src=/en/publication/zhuang-iros-2022/featured_hud7e3312f6e9bc8cbce5bc5e91c849d57_8860161_30b0cc2c98626471d030c2137e5324d4.webp height=79 width=150 alt="GoferBot: A Visual Guided Human-Robot Collaborative Assembly System" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=https://github.com/DavidZhang73/ImageCaptionGenerator target=_blank rel=noopener>Image Caption Generator</a></div><a href=https://github.com/DavidZhang73/ImageCaptionGenerator target=_blank rel=noopener class=summary-link><div class=article-style>An encoder(Resnet152)-decoder(LSTM) implementation of image caption model.</div></a><div class="stream-meta article-metadata"></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/DavidZhang73/ImageCaptionGenerator target=_blank rel=noopener>Code</a></div></div><div class=ml-3><a href=https://github.com/DavidZhang73/ImageCaptionGenerator target=_blank rel=noopener><img src=/en/project/image-caption-generator/featured_hua9b8deec36d6892c209521dce5147c5a_251706_523e28b983c2c5e38b15820152415102.webp height=69 width=150 alt="Image Caption Generator" loading=lazy></a></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2025 DavidZ. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target=_blank rel=noopener>Hugo Blox Builder</a> — the free, <a href=https://github.com/HugoBlox/hugo-blox-builder target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.f64289d8217e08e3afcd597d60836062.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script>
<script src=/en/js/wowchemy.min.b4aa18ca30bd61a574eeea72304f9545.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.9137013a66774049159934c29c3f0205.js type=module></script>
<script async defer data-website-id=78221282-8049-4f9e-9010-ee8baa093965 src=https://umami.davidz.cn/script.js></script></body></html>