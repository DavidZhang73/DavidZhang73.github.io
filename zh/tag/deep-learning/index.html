<!doctype html><!-- This site was created with Hugo Blox. https://hugoblox.com --><!-- Last Published: 2025年3月6日 --><html lang=zh-Hans><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Hugo Blox Builder 5.9.7"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.26c458e6907dc03073573976b7f4044e.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.f6689966c0a10712f95f034011917db0.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="张家豪"><meta name=description content="Academic Page abouth Jiahao Zhang"><link rel=alternate hreflang=en href=https://academic.davidz.cn/en/tag/deep-learning/><link rel=alternate hreflang=zh-hans href=https://academic.davidz.cn/zh/tag/deep-learning/><link rel=canonical href=https://academic.davidz.cn/zh/tag/deep-learning/><link rel=manifest href=/zh/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@academic-davidz"><meta property="twitter:creator" content="@academic-davidz"><meta property="twitter:image" content="https://academic.davidz.cn/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="og:type" content="website"><meta property="og:site_name" content="Academic-DavidZ"><meta property="og:url" content="https://academic.davidz.cn/zh/tag/deep-learning/"><meta property="og:title" content="Deep Learning | Academic-DavidZ"><meta property="og:description" content="Academic Page abouth Jiahao Zhang"><meta property="og:image" content="https://academic.davidz.cn/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="zh-Hans"><meta property="og:updated_time" content="2025-02-27T00:00:00+00:00"><link rel=alternate href=/zh/tag/deep-learning/index.xml type=application/rss+xml title=Academic-DavidZ><title>Deep Learning | Academic-DavidZ</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper><script src=/js/wowchemy-init.min.9e4214442a7711d35691acd58f6f6361.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>搜索</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=搜索... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=搜索...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/zh/>Academic-DavidZ</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label=切换导航>
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/zh/>Academic-DavidZ</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/zh/#about><span>主页</span></a></li><li class=nav-item><a class=nav-link href=/zh/#featured><span>文章</span></a></li><li class=nav-item><a class=nav-link href=/zh/#projects><span>项目</span></a></li><li class=nav-item><a class=nav-link href=/zh/#experience><span>经历</span></a></li><li class=nav-item><a class=nav-link href=/zh/#accomplishments><span>获奖</span></a></li><li class=nav-item><a class=nav-link href=https://cv.davidz.cn/ target=_blank rel=noopener><span>简历</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=搜索><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>浅色</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>深色</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>自动</span></a></div></li><li class="nav-item dropdown i18n-dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true aria-label=语言><i class="fas fa-globe mr-1" aria-hidden=true></i><span class="d-none d-lg-inline">中文 (简体)</span></a><div class=dropdown-menu><div class="dropdown-item dropdown-item-active"><span>中文 (简体)</span></div><a class=dropdown-item href=https://academic.davidz.cn/en/tag/deep-learning/><span>English</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class="universal-wrapper pt-3"><h1>Deep Learning</h1></div><div class=universal-wrapper><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/zh/publication/deng-cvpr-2025/>Pos3R: 6D Pose Estimation for Unseen Objects Made Easy</a></div><a href=/zh/publication/deng-cvpr-2025/ class=summary-link><div class=article-style>Foundation models have significantly reduced the need for task-specific training, while also enhancing generalizability. However, …</div></a><div class="stream-meta article-metadata"><div><span>Weijian Deng</span>, <span>Dylan Campbell</span>, <span>Chunyi Sun</span>, <span class=author-highlighted>张家豪</span>, <span>Shubham Kanitkar</span>, <span>Matthew E. Shaffer</span>, <span>Stephen Gould</span></div></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/zh/publication/deng-cvpr-2025/cite.bib>引用</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=https://github.com/DavidZhang73/Manual-PA target=_blank rel=noopener>Manual-PA</a></div><a href=https://github.com/DavidZhang73/Manual-PA target=_blank rel=noopener class=summary-link><div class=article-style>Manual-PA: Learning 3D Part Assembly from Instruction Diagrams 的官方实现.</div></a><div class="stream-meta article-metadata"></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/DavidZhang73/Manual-PA target=_blank rel=noopener>代码</a></div></div><div class=ml-3><a href=https://github.com/DavidZhang73/Manual-PA target=_blank rel=noopener><img src=/zh/project/manual-pa/featured_huaaffbd521d97088514c514884367b2b6_185219_c7a1c8afd83e33e4625bb6b61c99a203.webp height=71 width=150 alt=Manual-PA loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=https://github.com/DavidZhang73/TDGV target=_blank rel=noopener>TDGV</a></div><a href=https://github.com/DavidZhang73/TDGV target=_blank rel=noopener class=summary-link><div class=article-style>WACV 2025 Temporal Instructional Diagram Grounding in Unconstrained Videos 的官方实现.</div></a><div class="stream-meta article-metadata"></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/DavidZhang73/TDGV target=_blank rel=noopener>代码</a></div></div><div class=ml-3><a href=https://github.com/DavidZhang73/TDGV target=_blank rel=noopener><img src=/zh/project/tdgv/featured_hu81a49bbe34b252829c0d9263ba11984f_178158_f0d562fff67d51a916fdf792213ae115.webp height=58 width=150 alt=TDGV loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/zh/publication/zhang-arxiv-2025/>Manual-PA: Learning 3D Part Assembly from Instruction Diagrams</a></div><a href=/zh/publication/zhang-arxiv-2025/ class=summary-link><div class=article-style>这篇论文提出了一种基于手册的3D部件装配方法，旨在通过学习装配顺序和预测6D位姿，自动化实现复杂家具的组装。本文利用对比学习和变换器架构，设计了Manual-PA框架，以手册中的步骤图为指导，进行部件的语义对齐和装配顺序推理，结合位置编码优化装配过程。实验表明，该方法在PartNet和IKEA-Manual数据集上显著提升了装配性能，尤其是在复杂结构和多部件任务上的鲁棒性和泛化能力。未来工作可进一步探索更灵活的检测机制、多视角处理，以及统一的类别无关模型。(ChatGPT4o)</div></a><div class="stream-meta article-metadata"><div><span class=author-highlighted>张家豪</span>, <span>Anoop Cherian</span>, <span>Cristian Rodriguez</span>, <span>Weijian Deng</span>, <span>Stephen Gould</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2411.18011 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/zh/publication/zhang-arxiv-2025/cite.bib>引用</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/DavidZhang73/Manual-PA target=_blank rel=noopener>代码</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2411.18011 target=_blank rel=noopener>ArXiv</a></div></div><div class=ml-3><a href=/zh/publication/zhang-arxiv-2025/><img src=/zh/publication/zhang-arxiv-2025/featured_huc624b6be12134b14bcc82098be4060bc_208447_94830cd4f9876d7945680a7b54cd70ed.webp height=84 width=150 alt="Manual-PA: Learning 3D Part Assembly from Instruction Diagrams" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/zh/publication/zhang-wacv-2025/>Temporally Grounding Instructional Diagrams in Unconstrained Videos</a></div><a href=/zh/publication/zhang-wacv-2025/ class=summary-link><div class=article-style>这篇文章的主要研究对象是在视频中同时定位多个查询序列，尤其是将说明性图解与视频中的时间点对齐的问题。作者指出，现有的很多方法只针对单个查询进行定位，忽略了查询之间的内在关系（如互斥性和时间顺序），这可能导致不同步骤图解的时间跨度重叠或顺序错误，进而影响定位的准确性。为了应对这个问题，作者提出了一种新的方法，通过构造复合查询（将步骤图解的视觉内容特征与固定数量的可学习位置嵌入结合）来同时定位多个步骤图解。该方法通过自注意力机制减少时间跨度的重叠，并通过内容和位置的联合指导校正时间上的错位。文章展示了该方法在Ikea Assembly in the Wild（IAW）数据集和YouCook2基准数据集上的有效性，能够显著优于现有方法，同时能够同时定位多个查询。这种方法的核心贡献是设计了一种新的检测Transformer模型，能够同时定位一系列步骤图解，并通过复合查询和联合指导的交叉注意力机制提高定位准确性。(ChatGPT4o)</div></a><div class="stream-meta article-metadata"><div><span class=author-highlighted>张家豪</span>, <span>Frederic Zhang</span>, <span>Cristian Rodriguez</span>, <span>Yizhak Ben-Shabat</span>, <span>Anoop Cherian</span>, <span>Stephen Gould</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content/WACV2025/papers/Zhang_Temporally_Grounding_Instructional_Diagrams_in_Unconstrained_Videos_WACV_2025_paper.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/zh/publication/zhang-wacv-2025/cite.bib>引用</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/DavidZhang73/TDGV target=_blank rel=noopener>代码</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2407.12066 target=_blank rel=noopener>ArXiv</a></div></div><div class=ml-3><a href=/zh/publication/zhang-wacv-2025/><img src=/zh/publication/zhang-wacv-2025/featured_huf33ce902d26e13d5970a88b7113bb1eb_1108386_2976f462c2140895a7c228b42ea4ee00.webp height=50 width=150 alt="Temporally Grounding Instructional Diagrams in Unconstrained Videos" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=https://github.com/DavidZhang73/AssemblyVideoManualAlignment target=_blank rel=noopener>Assembly Video Manual Alignment</a></div><a href=https://github.com/DavidZhang73/AssemblyVideoManualAlignment target=_blank rel=noopener class=summary-link><div class=article-style>CVPR 2023 Aligning Step-by-Step Instructional Diagrams to Video Demonstrations 的官方实现。</div></a><div class="stream-meta article-metadata"></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/DavidZhang73/AssemblyVideoManualAlignment target=_blank rel=noopener>代码</a></div></div><div class=ml-3><a href=https://github.com/DavidZhang73/AssemblyVideoManualAlignment target=_blank rel=noopener><img src=/zh/project/assembly-video-manual-alignment/featured_hu0841f9d955ced15ce79790f0bb5508fd_132634_dcd1b8e79352e909525e936212f158ef.webp height=60 width=150 alt="Assembly Video Manual Alignment" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/zh/publication/zhang-cvpr-2023/>Aligning Step-by-Step Instructional Diagrams to Video Demonstrations</a></div><a href=/zh/publication/zhang-cvpr-2023/ class=summary-link><div class=article-style>文章讨论了如何对齐逐步说明图与现实世界中的视频演示，尤其是在家具组装场景下。传统的Ikea组装手册通常使用图解说明步骤，但这些图解有时可能含糊不清或与实际的产品存在差异，而通过视频来展示组装过程可以有效弥补这些不足。然而，网络上的DIY视频往往包含大量与实际任务无关的内容，这使得视频与说明图的对齐成为一个复杂的问题。为了解决这一问题，文章提出了一种新的对比学习框架，旨在通过多模态特征的对齐，将视频片段与图解进行匹配。文章引入了一个新的数据集——IAW（Ikea Assembly in the Wild），该数据集包含超过183小时的家具组装视频以及8000多张图解说明，用于评估该方法的效果。实验结果表明，该方法在视频片段和说明图解之间的检索和对齐任务上取得了显著的性能提升。此外，文章提出了三种针对性的新型损失函数，旨在通过对比学习更好地对齐视频和图解，包括视频与图解的全局对比损失、视频与手册的局部对比损失，以及手册内图解之间的对比损失。实验显示，使用这些损失函数可以显著提高模型的对齐性能。这项研究在视频-图解对齐、多模态对齐领域具有重要意义，尤其是在机器人模仿学习和人类组装任务辅助等应用中，具有广泛的潜在应用前景。(ChatGPT4o)</div></a><div class="stream-meta article-metadata"><div><span class=author-highlighted>张家豪</span>, <span>Anoop Cherian</span>, <span>Yanbin Liu</span>, <span>Yizhak Ben-Shabat</span>, <span>Cristian Rodriguez</span>, <span>Stephen Gould</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Aligning_Step-by-Step_Instructional_Diagrams_to_Video_Demonstrations_CVPR_2023_paper.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/zh/publication/zhang-cvpr-2023/cite.bib>引用</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/DavidZhang73/AssemblyVideoManualAlignment target=_blank rel=noopener>代码</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://iaw.davidz.cn target=_blank rel=noopener>数据集</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://cvpr2023.thecvf.com/media/PosterPDFs/CVPR%202023/22280.png target=_blank rel=noopener>海报</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://cvpr2023.thecvf.com/media/cvpr-2023/Slides/22280.pdf target=_blank rel=noopener>演示文稿</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=8iC5QyP8U6o" target=_blank rel=noopener>视频</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/CVPR52729.2023.00245 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2303.13800 target=_blank rel=noopener>ArXiv</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=/zh/publication/zhang-cvpr-2023/supplementary.pdf>补充材料</a></div></div><div class=ml-3><a href=/zh/publication/zhang-cvpr-2023/><img src=/zh/publication/zhang-cvpr-2023/featured_hu47d4830ae752520931f9bd070df3961b_714524_4d94f7642c4f078616285f6282fdcb97.webp height=66 width=150 alt="Aligning Step-by-Step Instructional Diagrams to Video Demonstrations" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/zh/publication/zhuang-iros-2022/>GoferBot: A Visual Guided Human-Robot Collaborative Assembly System</a></div><a href=/zh/publication/zhuang-iros-2022/ class=summary-link><div class=article-style>这篇文章介绍了一种名为GoferBot的视觉引导人机协作装配系统，旨在应对智能制造环境中人机协作的需求。文章中提出的GoferBot系统主要通过视觉感知来识别人类的动作和任务进展，并进行相应的反应。该系统包括三个子模块：视觉伺服模块、动作识别模块和视觉交接模块。GoferBot能够在动态且非结构化的环境中完成装配任务，例如通过识别和预测人类动作来实现零件的抓取和交接。研究表明，该系统在不依赖深度传感器、标记物或运动追踪器的情况下，仅使用两个RGB摄像头即可实现高效的协同工作。通过实验验证，GoferBot在家具装配任务（例如IKEA桌子装配）中表现出良好的效率，并且与基于语音指令的系统相比，人机交互更加直观自然。文章还讨论了GoferBot的评估方法，分为以机器人为中心的评估和以人为中心的评估。实验结果表明，GoferBot在多个装配循环中达到了90%的成功率，尽管在重复任务中系统的性能会有所下降。文章最后总结了GoferBot的局限性，并提出了未来改进方向。(ChatGPT4o)</div></a><div class="stream-meta article-metadata"><div><span>Zheyu Zhuang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title=相同贡献></i>, <span>Yizhak Ben-Shabat</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title=相同贡献></i>, <span class=author-highlighted>张家豪</span>, <span>Stephen Gould</span>, <span>Robert Mahony</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://ieeexplore.ieee.org/abstract/document/9981122 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/zh/publication/zhuang-iros-2022/cite.bib>引用</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=Fo5XI5OJ4QQ" target=_blank rel=noopener>视频</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/IROS47612.2022.9981122 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2304.08840 target=_blank rel=noopener>ArXiv</a></div></div><div class=ml-3><a href=/zh/publication/zhuang-iros-2022/><img src=/zh/publication/zhuang-iros-2022/featured_hud7e3312f6e9bc8cbce5bc5e91c849d57_8860161_30b0cc2c98626471d030c2137e5324d4.webp height=79 width=150 alt="GoferBot: A Visual Guided Human-Robot Collaborative Assembly System" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=https://github.com/DavidZhang73/ImageCaptionGenerator target=_blank rel=noopener>Image Caption Generator</a></div><a href=https://github.com/DavidZhang73/ImageCaptionGenerator target=_blank rel=noopener class=summary-link><div class=article-style>一个基于 encoder(Resnet152)-decoder(LSTM) 的图像描述模型实现。</div></a><div class="stream-meta article-metadata"></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/DavidZhang73/ImageCaptionGenerator target=_blank rel=noopener>代码</a></div></div><div class=ml-3><a href=https://github.com/DavidZhang73/ImageCaptionGenerator target=_blank rel=noopener><img src=/zh/project/image-caption-generator/featured_hua9b8deec36d6892c209521dce5147c5a_251706_523e28b983c2c5e38b15820152415102.webp height=69 width=150 alt="Image Caption Generator" loading=lazy></a></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2025 DavidZ. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>由<a href="https://hugoblox.com/?utm_campaign=poweredby" target=_blank rel=noopener>Hugo Blox Builder</a>支持发布——免费<a href=https://github.com/HugoBlox/hugo-blox-builder target=_blank rel=noopener>开源</a>网站，为创作者赋能。</p></footer></div></div><script src=/js/vendor-bundle.min.f64289d8217e08e3afcd597d60836062.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script>
<script src=/zh/js/wowchemy.min.969c059ff7aed5c4de7aa0431d7a9a30.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>引用</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> 复制</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> 下载</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.9137013a66774049159934c29c3f0205.js type=module></script>
<script async defer data-website-id=78221282-8049-4f9e-9010-ee8baa093965 src=https://umami.davidz.cn/script.js></script></body></html>